---
title: A Comparision of tidytext and tm
author: Sean Warlick
date: '2017-11-26'
slug: a-comparision-of-tidytext-and-tm
categories:
  - R
tags:
  - tidyverse
  - tm
draft: true
---

# Introduction
In our previous post we explored the data structures utilized in **tm** and **tidytext**.  In this article we compare methods for cleaning and normalizing text with each package.  For this post well focus on striping numbers and punctuation, converting to lower case, tokenization and stemming or lemitization.  

# Data
For this exploration we will again be using a sample of 19 documents from the [Gutenburg Collection](http://www.gutenberg.org).  The Gutenberg collection can be accessed in R using the **gutenbergr** package.  

The data for this example is a data frame with four columns and nineteen rows. Each row represents a document.  The text of the document is in the *full_text* variable.  The remaining columns - *id*, *author*, and *title* - provide metadata for the given document.  

```{r data_collect, include=FALSE}
library(tm)
library(knitr)
library(dplyr)
library(gutenbergr)

set.seed(42)

# Obtain df with gutenberg ids of 20 works from Mark Twain or Shakespear.  
document_id <- gutenberg_works() %>% 
    filter(author %in% c('Twain, Mark', 'Shakespear, William') 
           & language == 'en'
           & has_text == TRUE) %>%
    sample_n(20) %>% 
    select(gutenberg_id)

# Downlaod the documents
gutenberg_data <- gutenberg_download(document_id$gutenberg_id,
                   meta_fields = c('title', 'author'))

# Convert Returned Data so there is only 1 row per text. 
text_data<- c()
for(id in unique(gutenberg_data$gutenberg_id)){
    
    # Filter to single document
    current_document <- gutenberg_data %>% 
        filter(gutenberg_id == id) 
    
    ## Get Title & Author for the document
    current_title <- current_document %>% distinct(title)
    title <- current_title$title
    current_author <- current_document %>% distinct(author)
    author <- current_author$author
    
    ## Combine all text ojbects
    combined_text <- paste(current_document$text, collapse = ' ')
    ## Convert to data frame
    text_data <- rbind(text_data, c(id, author, title, combined_text))
}

text_data <- as.data.frame(text_data)
names(text_data) <- c('id', 'author', 'title', 'full_text')
text_data <- as.tbl(text_data)

tidy_text <- text_data %>% select(-id)
```  

# Cleaning With **tm**
The **tm** package provides a full set of functions to handle standard text normalization and cleaning.  The cleaning functions are invoked using the `tm_map()` function.  Cleaning functions not included in **tm** can also be invoked using `content_transformer()`.  

We'll start our cleaning process by performing the very basic steps of converting all the text to lower case, then remove any numbers in the text and then remove the punctuation.  The first step of the process uses `base::tolower()` so we need to wrap the function in `content_transformer()`.
```{r tm_corpus, include=FALSE}
# Create A Corpus
corpus <- VCorpus(VectorSource(text_data$full_text))

# Assign Metadata
meta(corpus, type = 'indexed', 'tag' = 'author') <- text_data$author
meta(corpus, type = 'indexed', 'tag' = 'title') <- text_data$title
```  
```{r tm_cleaning} 
# Original Corpus
substring(corpus[[1]]$content, 1, 250)

# Cleaning of Corpus
corpus2 <- corpus %>% 
    tm_map(., content_transformer(tolower)) %>% 
    tm_map(., removeNumbers) %>% 
    tm_map(., removePunctuation) %>% 
    tm_map(., stripWhitespace)

substring(corpus2[[1]]$content, 1, 250)
```  

Notice that as a result of the cleaning process that *Note.--I* ended up as 
*notei* due to the removal of the punctuation.  This is a great example of how 
the cleaning process is very iterative.  To correct this bug, let's define a 
custom function to replace dashes with a space and introduce it into the 
cleaning pipeline.  To use our function with `tm_map()` we will need to again 
wrap it with `content_transformer()`.  

```{r custom_function}
replace_dash <- function(x){
    gsub('-+', ' ', x)
} 

corpus2 <- corpus %>% 
    tm_map(., content_transformer(tolower)) %>% 
    tm_map(., removeNumbers) %>%
    tm_map(., content_transformer(replace_dash)) %>% 
    tm_map(., removePunctuation) %>% 
    tm_map(., stripWhitespace)

substring(corpus2[[1]]$content, 1, 250)
```  
Now that we have removed the non-word characters, we can start to normalize the 
text.  We'll start by removing the most common words which wouldn't contribute
to the analysis.  We then use stemming to remove common endings to reduce the 
words to their root.  
```{r remove_stop_words}  
# Remove stopwords and stem the document
corpus3 <- corpus2 %>% 
    tm_map(., removeWords, stopwords()) %>% 
    tm_map(., stemDocument, language = 'porter')

substring(corpus3[[1]]$content, 1, 250)
```  
  
## Cleaning with **tidytext**  
The **tidytext** package is very signle minded in reaching it's very specific
goal, to make it easy to apply tidy data princpals to text analytics.  However, this single mindedness means that the package doesn't include any functions to
prepare the text.  However, the tools included **tm** and used above work well
with **dplyr** and **tidytext**.  
  
As result of this compatibility, the code to prepare the documents in the **tidytext** paradime is very similar to the code in the examples above.  The 
biggest difference is that we don't need to use `content_transformer()` to work
with `tolower()` or our `replace_dash()` function.  
```{r tidy_text}  
tidy_text %>% 
    mutate(clean_text = tolower(full_text)) %>% 
    mutate(clean_text = replace_dash(clean_text)) %>% 
    mutate(clean_text = removeNumbers(clean_text)) %>% 
    mutate(clean_text = removePunctuation(clean_text)) %>% 
    mutate(clean_text = removeWords(clean_text,
                                    stopwords('english')
                                    )
           ) %>% 
    mutate(clean_text = stemDocument(clean_text)) %>% 
    select(clean_text) %>%
    head()
```
